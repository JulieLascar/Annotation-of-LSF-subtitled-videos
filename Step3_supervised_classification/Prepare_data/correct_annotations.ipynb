{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correction of annotations in cases of signs overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, r\"/home/jlascar/Documents/LSF_annotation/Step3_supervised_classification\")\n",
    "os.chdir(sys.path[0])\n",
    "from data_utils import *\n",
    "\n",
    "sys.path.append(\"Training/\")\n",
    "from training_utils import smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load files from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Mediapi_Expert/\"\n",
    "dict_path = \"/home/jlascar/Documents/data/Mediapi/dictionnaire_expert\"\n",
    "\n",
    "d_gloses2Gid = pickle.load(open(dataset_name + \"saved_files/d_gloses2Gid.pkl\", \"rb\"))\n",
    "d_Gid2gloses = pickle.load(open(dataset_name + \"saved_files/d_Gid2gloses.pkl\", \"rb\"))\n",
    "d_Gid2Vid = pickle.load(open(dataset_name + \"saved_files/d_Gid2Vid.pkl\", \"rb\"))\n",
    "d_Vid2Labels = pickle.load(open(dataset_name + \"saved_files/d_Vid2Labels.pkl\", \"rb\"))\n",
    "d_Vid2Gid = pickle.load(open(dataset_name + \"saved_files/d_Vid2Gid.pkl\", \"rb\"))\n",
    "L_videos = pickle.load(open(dataset_name + \"saved_files/L_videos.pkl\", \"rb\"))\n",
    "ex_to_look = pickle.load(open(dataset_name + \"saved_files/ex_to_look.pkl\", \"rb\"))\n",
    "dico_cropId2subtitle = pickle.load(open(\"dico_cropId2subtitle.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'vendredi_dernier',\n",
       " 2: 'vendredi',\n",
       " 3: 'septembre',\n",
       " 4: 'jeudi dernier',\n",
       " 5: 'octobre_1',\n",
       " 6: 'mars_0',\n",
       " 7: 'france',\n",
       " 8: 'vendredi_prochain',\n",
       " 9: 'avril_0',\n",
       " 10: 'janvier_2',\n",
       " 11: 'mercredi_prochain',\n",
       " 12: 'etats-unis_0',\n",
       " 13: 'novembre_0',\n",
       " 14: 'lundi_dernier',\n",
       " 15: 'canada_1',\n",
       " 16: 'japon',\n",
       " 17: 'allemagne',\n",
       " 18: 'fevrier',\n",
       " 19: 'mardi_dernier',\n",
       " 20: 'decembre_0',\n",
       " 21: 'canada_0',\n",
       " 22: 'juillet_2',\n",
       " 23: 'espagne_0',\n",
       " 24: 'lundi_0',\n",
       " 25: 'janvier_0',\n",
       " 26: 'angleterre',\n",
       " 27: 'juillet_1',\n",
       " 28: 'italie_0',\n",
       " 29: 'samedi',\n",
       " 30: 'mardi',\n",
       " 31: 'mai',\n",
       " 32: 'janvier_1',\n",
       " 33: 'mercredi_dernier',\n",
       " 34: 'octobre_0',\n",
       " 35: 'avril_1',\n",
       " 36: 'juillet_0',\n",
       " 37: 'jeudi_0',\n",
       " 38: 'mardi_prochain',\n",
       " 39: 'decembre_1',\n",
       " 40: 'juin',\n",
       " 41: 'aout',\n",
       " 42: 'samedi_dernier',\n",
       " 43: 'RV',\n",
       " 44: 'novembre_1',\n",
       " 0: 'neutre'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_Gid2gloses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotated videos : 1613\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of annotated videos :\", len(L_videos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Look at the overlapping cases and choose the right annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_errors(glose, v_capture_id, dict_path, dico_cropId2subtitle, d_Vid2Labels, d_gloses2Gid):\n",
    "    print(v_capture_id)\n",
    "    v_id = \"_\".join(v_capture_id.split(\"_\")[:-1])\n",
    "    print(dico_cropId2subtitle[v_id])\n",
    "    labels = d_Vid2Labels[v_id]\n",
    "    print(d_Vid2Labels[v_id])\n",
    "    df_glose = pd.read_csv(f\"{dict_path}/{glose}/confidence_scores.csv\")  # load csv with glose location\n",
    "    start = df_glose[df_glose.video_ids == v_capture_id].start_frame.item()\n",
    "    end = df_glose[df_glose.video_ids == v_capture_id].end_frame.item()\n",
    "    labels2 = labels.copy()\n",
    "    labels2[start : end + 1] = d_gloses2Gid[glose]\n",
    "    print(d_gloses2Gid[glose], \" : \", glose)\n",
    "    print(labels2)\n",
    "    return start, end\n",
    "\n",
    "\n",
    "def correct_annotations(ex_to_look):\n",
    "    for i in range(len(ex_to_look)):\n",
    "        glose, v_capture_id = ex_to_look[i]\n",
    "        start, end = analyse_errors(glose, v_capture_id, dict_path, dico_cropId2subtitle, d_Vid2Labels, d_gloses2Gid)\n",
    "        v_id = \"_\".join(v_capture_id.split(\"_\")[:-1])\n",
    "        r = input(\"Do you want first(1) ou second annotation(2) or delete a label(3)?\")\n",
    "        if r == \"2\":\n",
    "            d_Vid2Labels[v_id][start : end + 1] = d_gloses2Gid[glose]\n",
    "\n",
    "        elif r == \"3\":\n",
    "            s = input(\"which gloseId\")\n",
    "            d_Vid2Labels[v_id] = delete_labels(v_id, s)\n",
    "\n",
    "\n",
    "def delete_labels(v_id, label):\n",
    "    labels = d_Vid2Labels[v_id]\n",
    "    labels_corrected = np.where(labels == label, 0, labels)\n",
    "    return labels_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357b184b8d_0048_0\n",
      "Sept conseillers municipaux sourds ont été élus ou réélus, lors des élections municipales de mars et juin dernier.\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 6. 6. 6. 6. 6.\n",
      " 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "40  :  juin\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6. 40. 40. 40. 40. 40.\n",
      " 40. 40.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "correct_annotations([ex_to_look[i]])\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Smooth the annotations and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing\n",
    "for v_id in L_videos:\n",
    "    Ltemp = smoothing(d_Vid2Labels[v_id], len(d_Gid2gloses))\n",
    "    if Ltemp != list(d_Vid2Labels[v_id]):\n",
    "        print(d_Vid2Labels[v_id])\n",
    "        print(Ltemp)\n",
    "        d_Vid2Labels[v_id] = np.array(Ltemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(d_Vid2Labels, open(dataset_name + \"saved_files/d_Vid2Labels.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
